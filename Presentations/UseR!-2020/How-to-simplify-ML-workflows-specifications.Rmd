---
title: "How to simplify Machine learning workflows specifications?"
author: "Anton Antonov"
date: '2020-02-02'
output:
  pdf_document: default
  html_notebook: default
---

```{r,echo=F}
library(SMRMon)
library(SparseMatrixRecommender)
library(LSAMon)
library(QRMon)
library(Matrix)
library(magrittr)
library(ExternalParsersHookUp)
```

```{r,echo=F}
source("./Additional-definitions.R")
```


# Abstract 

In this presentation we discuss a systematic approach of software development
that gives us the ability to rapidly specify Machine Learning (ML) computations
using both programming Domain Specific Languages (DSL's) and natural language commands.
We present in detail the selection of programming paradigms, languages, and packages.

A central topic of the presentation is the transformation of sequences of natural 
commands into corresponding DSL pipelines for ML computations. 
We discuss the general strategy and the concrete implementation steps.

The presented approach is based on
(1) monadic programming DSL's, 
(2) finite state machines, and
(3) natural language context free grammars.

We use monadic programming and code generation for implementation of ML packages.
We use Raku (Perl 6) for grammar specifications, parsers generation, and interpreters.

Numerous examples are used for illustration and clarification purposes based on 
ML packages written in R and English-based grammar rules. We look into code generation 
of ML workflows for supervised learning, time series analysis, latent semantic analysis, 
and recommendations. We show how with the same natural commands pipelines in other
programming languages can be generated.

Finally we discuss the extensions of the presented approach to (1) handling wrong commands and
spelling mistakes, (2) using multiple natural languages, and (3) making conversational agents.

Here is an example of a natural language specification for the code generation of 
a recommendation system pipeline (with an intentional misspelling):

```{r}
to_SMRMon_R_command( 
    "create from dfTitanic;
     apply the LSI functions IDF, TermFrequency, Cosine;
     recommend for histry id.5, id.7; 
     join across recommendations with the data frame dfTitanic; 
     echo pipeline value")
```

# The big picture

This diagram shows the general pattern followed for making of the conversational agents 
that we discuss in the presentation:

![MonadicMakingOfMLConversationalAgents](https://github.com/antononcube/ConversationalAgents/raw/master/ConceptualDiagrams/Monadic-making-of-ML-conversational-agents.jpg)

# ML workflows by natural commands

## Recommendations

Here we interpret a sequence of natural commands into a recommender pipeline, [AAp1]:

```{r}
to_SMRMon_R_command( 
    "create from dfTitanic;
     apply IDF, TermFrequency, Cosine;
     recommend for history id.5, id.7; 
     join across recommendations with the data frame dfTitanic; 
     echo pipeline value")
```

Here are the evaluation results of the generated pipeline:

```{r}
smrObj <- 
  eval( expr = to_SMRMon_R_command( 
    "create from dfTitanic;
     apply IDF, TermFrequency, Cosine;
     recommend for history id.5, id.7; 
     join across recommendations with the data frame dfTitanic; 
     echo pipeline value" ) )
```

## Latent semantic analysis

Get rstudio::conf 2019 abstracts:

```{r}
dfRSCAbstracts <- read.csv( "https://raw.githubusercontent.com/antononcube/SimplifiedMachineLearningWorkflows-book/master/Data/RStudio-conf-2019-abstracts.csv", stringsAsFactors = FALSE )
lsAbstracts <- setNames( dfRSCAbstracts$Abstract, dfRSCAbstracts$ID )
```

Specify an LSA pipeline and evaluate it, [AAp2]:

```{r}
set.seed(942)
lsaObj <-
  eval( expr = to_LSAMon_R_command( 
  "create from lsAbstracts;
   make document term matrix with stemming FALSE and automatic stop words;
   apply LSI functions global weight function idf, local term weight function none, normalizer function cosine;
   extract 16 topics using method NNMF and max steps 12;
   show topics table with 12 columns and 10 terms;" ) )
```

Get statistical thesaurus:

```{r}
lsaObj <- 
eval( expr = to_LSAMon_R_command( "use lsa object lsaObj; 
                                   show thesaurus table for dplyr, shiny, tidyverse;" ))
```

## Quantile regression

Specify a Quantile Regression (QR) workflow, [AAp3]:

```{r}
qrObj <-
  eval( expr = to_QRMon_R_command( 
    "create from dfTemperatureData;
     compute quantile regression with 12 knots;
     show date plot with date origin 1900-01-01" ) )
```


# The general strategy

The particular approach we follow has the following steps.

1. Design and program a software monad for a certain type of ML workflows.
   - In a programming language that allows such definition.
     - Like R and WL.
   - Referred to as "Machine Learning Monad (MLMon)" below.
   - For example, program MLMon for Latent Semantic Analysis (LSA) or Quantile Regression (QR).
     
2. Create and systematically run unit tests for MLMon.
     
3. Write down a context free grammar for the targeted ML workflows.
   - In a natural language of choice. Say, English.
   
4. Create and systematically run unit tests for the parsing of the specified MLMon grammar.

5. Create an interpreter of natural language parsed commands into MLMon software code.

6. Create and systematically run unit tests for the interpreter.

7. Create programming tools that facilitate the creation of such software monads and related grammars.


# How to concretely do it?

- Make software monad generation packages. For example:

  - [StateMonadCodeGenerator-R](https://github.com/antononcube/R-packages/tree/master/StateMonadCodeGenerator);
  - [StateMonadCodeGenerator-WL](https://github.com/antononcube/MathematicaForPrediction/blob/master/MonadicProgramming/StateMonadCodeGenerator.m).

- Using [Raku (Perl 6)](https://www.raku.org) for:
  - the natural language commands grammar parsers;
  - the programming code interpreters.
  
 

# References

### Repositories

[AAr1] Anton Antonov, [R-packages](https://github.com/antononcube/R-packages), (2019), GitHub.

[AAr2] Anton Antonov, [Conversational Agents](https://github.com/antononcube/ConversationalAgents), (2017), GitHub.

### Packages

[AAp1] Anton Antonov, 
[Sparse Matrix Recommender Monad in R](https://github.com/antononcube/R-packages/tree/master/SMRMon-R), 
(2019),
[R-packages at GitHub](https://github.com/antononcube/R-packages).

[AAp2] Anton Antonov, 
[Latent Semantic Analysis Monad in R](https://github.com/antononcube/R-packages/tree/master/LSAMon-R), 
(2019),
[R-packages at GitHub](https://github.com/antononcube/R-packages).

[AAp3] Anton Antonov, 
[Quantle Regression Monad in R](https://github.com/antononcube/QRMon-R), 
(2019),
GitHub.

