# Wolfram U Latent Semantic Analysis workflows lectures

## In brief

The lectures on 
[Latent Semantic Analysis (LSA)](https://en.wikipedia.org/wiki/Latent_semantic_analysis) 
are to be recorded through Wolfram University (Wolfram U) in December 2019 and January-February 2020.


## The lectures (as live-coding sessions)

1. [X] Overview Latent Semantic Analysis (LSA) typical problems and basic workflows.    
   Answering preliminary anticipated questions.     
   Here is 
   [the recording of the first session at Twitch](https://www.twitch.tv/videos/517562647) .
   
   - What are the typical applications of LSA?   
   - Why use LSA?     
   - What it the fundamental philosophical or scientific assumption for LSA?   
   - What is the most important and/or fundamental step of LSA?   
   - What is the difference between LSA and Latent Semantic Indexing (LSI)?   
   - What are the alternatives?
     - Using Neural Networks instead?   
   - How is LSA used to derive similarities between two given texts?   
   - How is LSA used to evaluate the proximity of phrases?
     (That have different words, but close semantic meaning.)   
   - How the main dimension reduction methods compare?
      
2. [ ] LSA for document collections.

   - Representation of the documents.
     - The fundamental matrix object.
   - Dimension reduction methods comparisons.
   - Representation of unseen documents.
   - Similarity between two text: 
     - the "local world" perspective, and 
     - the "bigger universe" perspective.
   
3. [ ] LSA for image collections.

   - Representation of the documents.
   - Dimension reduction methods comparisons.
   - Representation of unseen documents.
   
4. [ ] Further use cases.

   - LSA based image classification.
   - LSA for time series collections.
